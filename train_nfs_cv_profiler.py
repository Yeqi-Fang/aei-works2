# train_bottleneck_analyzer.py - æ™ºèƒ½ç“¶é¢ˆåˆ†æå™¨

from __future__ import annotations
import warnings
warnings.filterwarnings("ignore", category=FutureWarning, module="torch")

import argparse
from pathlib import Path
from typing import Tuple, List, Dict, Optional
import os
from datetime import datetime
import time
import threading
import subprocess
import psutil
import gc
from collections import deque
import json

import numpy as np
import pandas as pd
import torch
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.profiler import profile, ProfilerActivity, record_function
import matplotlib.pyplot as plt

from nflows.distributions.normal import StandardNormal
from nflows.flows.base import Flow
from nflows.transforms.base import CompositeTransform
from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform
from nflows.transforms.permutations import RandomPermutation

# -----------------------------------------------------------------------------
# é«˜çº§ç“¶é¢ˆåˆ†æå™¨
# -----------------------------------------------------------------------------

class BottleneckAnalyzer:
    def __init__(self, log_dir: Path, sample_interval: float = 0.5):
        self.log_dir = log_dir
        self.profiler_dir = log_dir / "bottleneck_analysis"
        self.profiler_dir.mkdir(exist_ok=True)
        
        self.sample_interval = sample_interval
        self.monitoring = False
        self.monitor_thread = None
        
        # æ€§èƒ½æ•°æ®æ”¶é›†
        self.gpu_metrics = deque(maxlen=1000)
        self.cpu_metrics = deque(maxlen=1000)
        self.io_metrics = deque(maxlen=1000)
        self.memory_metrics = deque(maxlen=1000)
        
        # ç“¶é¢ˆæ£€æµ‹é˜ˆå€¼
        self.thresholds = {
            'gpu_util_low': 70,      # GPUåˆ©ç”¨ç‡ä½äº70%è®¤ä¸ºæ˜¯ç“¶é¢ˆ
            'cpu_util_high': 80,     # CPUåˆ©ç”¨ç‡é«˜äº80%è®¤ä¸ºæ˜¯ç“¶é¢ˆ
            'memory_util_high': 85,  # å†…å­˜ä½¿ç”¨ç‡é«˜äº85%è®¤ä¸ºæ˜¯ç“¶é¢ˆ
            'io_wait_high': 10,      # IOç­‰å¾…æ—¶é—´é«˜äº10%è®¤ä¸ºæ˜¯ç“¶é¢ˆ
        }
        
        # æ—¶é—´æµ‹é‡
        self.timings = {}
        self.current_phase = None
        
    def start_monitoring(self):
        """å¼€å§‹åå°ç›‘æ§"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        print("ğŸ” å¼€å§‹å®æ—¶æ€§èƒ½ç›‘æ§...")
        
    def stop_monitoring(self):
        """åœæ­¢åå°ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2)
        print("â¹ï¸  åœæ­¢æ€§èƒ½ç›‘æ§")
        
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                timestamp = time.time()
                
                # GPUç›‘æ§
                gpu_metrics = self._get_gpu_metrics()
                if gpu_metrics:
                    gpu_metrics['timestamp'] = timestamp
                    gpu_metrics['phase'] = self.current_phase
                    self.gpu_metrics.append(gpu_metrics)
                
                # CPUç›‘æ§
                cpu_metrics = self._get_cpu_metrics()
                cpu_metrics['timestamp'] = timestamp
                cpu_metrics['phase'] = self.current_phase
                self.cpu_metrics.append(cpu_metrics)
                
                # å†…å­˜ç›‘æ§
                memory_metrics = self._get_memory_metrics()
                memory_metrics['timestamp'] = timestamp
                memory_metrics['phase'] = self.current_phase
                self.memory_metrics.append(memory_metrics)
                
                # IOç›‘æ§
                io_metrics = self._get_io_metrics()
                io_metrics['timestamp'] = timestamp
                io_metrics['phase'] = self.current_phase
                self.io_metrics.append(io_metrics)
                
                time.sleep(self.sample_interval)
                
            except Exception as e:
                print(f"ç›‘æ§å‡ºé”™: {e}")
                
    def _get_gpu_metrics(self) -> Optional[Dict]:
        """è·å–GPUæŒ‡æ ‡"""
        if not torch.cuda.is_available():
            return None
            
        try:
            # PyTorch GPUå†…å­˜
            gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3
            gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3
            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            # å°è¯•è·å–GPUåˆ©ç”¨ç‡ (éœ€è¦nvidia-ml-pyæˆ–nvidia-smi)
            gpu_utilization = 0
            gpu_memory_util = 0
            try:
                result = subprocess.run([
                    'nvidia-smi', '--query-gpu=utilization.gpu,utilization.memory,temperature.gpu',
                    '--format=csv,noheader,nounits'
                ], capture_output=True, text=True, timeout=1)
                
                if result.returncode == 0:
                    values = result.stdout.strip().split(',')
                    gpu_utilization = float(values[0])
                    gpu_memory_util = float(values[1])
                    gpu_temperature = float(values[2])
                else:
                    gpu_temperature = 0
            except:
                gpu_temperature = 0
                
            return {
                'utilization': gpu_utilization,
                'memory_allocated': gpu_memory_allocated,
                'memory_reserved': gpu_memory_reserved,
                'memory_total': gpu_memory_total,
                'memory_utilization': gpu_memory_util,
                'temperature': gpu_temperature
            }
        except Exception:
            return None
            
    def _get_cpu_metrics(self) -> Dict:
        """è·å–CPUæŒ‡æ ‡"""
        cpu_percent = psutil.cpu_percent(interval=None)
        cpu_count = psutil.cpu_count()
        
        # è·å–è´Ÿè½½å¹³å‡å€¼
        try:
            load_avg = os.getloadavg()
        except:
            load_avg = [0, 0, 0]
            
        return {
            'utilization': cpu_percent,
            'count': cpu_count,
            'load_1min': load_avg[0],
            'load_5min': load_avg[1],
            'load_15min': load_avg[2]
        }
        
    def _get_memory_metrics(self) -> Dict:
        """è·å–å†…å­˜æŒ‡æ ‡"""
        memory = psutil.virtual_memory()
        return {
            'total_gb': memory.total / 1024**3,
            'used_gb': memory.used / 1024**3,
            'available_gb': memory.available / 1024**3,
            'percent': memory.percent
        }
        
    def _get_io_metrics(self) -> Dict:
        """è·å–IOæŒ‡æ ‡"""
        try:
            io_counters = psutil.disk_io_counters()
            if io_counters:
                return {
                    'read_bytes': io_counters.read_bytes,
                    'write_bytes': io_counters.write_bytes,
                    'read_time': io_counters.read_time,
                    'write_time': io_counters.write_time
                }
        except:
            pass
            
        return {
            'read_bytes': 0,
            'write_bytes': 0,
            'read_time': 0,
            'write_time': 0
        }
        
    def set_phase(self, phase_name: str):
        """è®¾ç½®å½“å‰é˜¶æ®µ"""
        self.current_phase = phase_name
        print(f"ğŸ“Š è¿›å…¥é˜¶æ®µ: {phase_name}")
        
    def time_phase(self, phase_name: str):
        """æµ‹é‡é˜¶æ®µæ‰§è¡Œæ—¶é—´çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        class PhaseTimer:
            def __init__(self, analyzer, name):
                self.analyzer = analyzer
                self.name = name
                self.start_time = None
                
            def __enter__(self):
                self.start_time = time.time()
                self.analyzer.set_phase(self.name)
                return self
                
            def __exit__(self, exc_type, exc_val, exc_tb):
                duration = time.time() - self.start_time
                self.analyzer.timings[self.name] = duration
                print(f"â±ï¸  {self.name}: {duration:.2f}s")
                self.analyzer.set_phase(None)
                
        return PhaseTimer(self, phase_name)
        
    def analyze_bottlenecks(self) -> Dict:
        """åˆ†ææ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = {
            'gpu_bottleneck': False,
            'cpu_bottleneck': False,
            'memory_bottleneck': False,
            'io_bottleneck': False,
            'data_loading_bottleneck': False,
            'recommendations': []
        }
        
        # åˆ†æGPUç“¶é¢ˆ
        if self.gpu_metrics:
            avg_gpu_util = np.mean([m['utilization'] for m in self.gpu_metrics if m.get('utilization', 0) > 0])
            max_gpu_memory = max([m['memory_allocated'] for m in self.gpu_metrics])
            
            if avg_gpu_util < self.thresholds['gpu_util_low']:
                bottlenecks['gpu_bottleneck'] = True
                bottlenecks['recommendations'].append(f"ğŸ”¥ GPUåˆ©ç”¨ç‡ä½ ({avg_gpu_util:.1f}%) - å»ºè®®å¢å¤§batch_sizeæˆ–æ¨¡å‹å¤æ‚åº¦")
                
            if max_gpu_memory < 2:  # å°äº2GB
                bottlenecks['recommendations'].append(f"ğŸ’¾ GPUå†…å­˜ä½¿ç”¨åä½ ({max_gpu_memory:.1f}GB) - å¯ä»¥å¢å¤§æ¨¡å‹æˆ–batch_size")
                
        # åˆ†æCPUç“¶é¢ˆ
        if self.cpu_metrics:
            avg_cpu_util = np.mean([m['utilization'] for m in self.cpu_metrics])
            max_cpu_util = max([m['utilization'] for m in self.cpu_metrics])
            
            if avg_cpu_util > self.thresholds['cpu_util_high']:
                bottlenecks['cpu_bottleneck'] = True
                bottlenecks['recommendations'].append(f"ğŸ–¥ï¸  CPUåˆ©ç”¨ç‡é«˜ ({avg_cpu_util:.1f}%) - å¯èƒ½æ˜¯æ•°æ®åŠ è½½ç“¶é¢ˆ")
                
        # åˆ†æå†…å­˜ç“¶é¢ˆ
        if self.memory_metrics:
            max_memory_percent = max([m['percent'] for m in self.memory_metrics])
            
            if max_memory_percent > self.thresholds['memory_util_high']:
                bottlenecks['memory_bottleneck'] = True
                bottlenecks['recommendations'].append(f"ğŸ’¾ å†…å­˜ä½¿ç”¨ç‡é«˜ ({max_memory_percent:.1f}%) - è€ƒè™‘å‡å°batch_size")
                
        # åˆ†ææ•°æ®åŠ è½½ç“¶é¢ˆ
        training_time = self.timings.get('è®­ç»ƒ', 0)
        data_loading_time = self.timings.get('æ•°æ®åŠ è½½', 0)
        
        if data_loading_time > 0 and training_time > 0:
            data_loading_ratio = data_loading_time / (training_time + data_loading_time)
            if data_loading_ratio > 0.2:  # æ•°æ®åŠ è½½å æ€»æ—¶é—´20%ä»¥ä¸Š
                bottlenecks['data_loading_bottleneck'] = True
                bottlenecks['recommendations'].append(f"ğŸ“ æ•°æ®åŠ è½½æ…¢ ({data_loading_ratio*100:.1f}%) - å¢åŠ num_workersæˆ–ä½¿ç”¨pin_memory")
                
        # ç»¼åˆåˆ†æ
        if not any([bottlenecks['gpu_bottleneck'], bottlenecks['cpu_bottleneck'], 
                   bottlenecks['memory_bottleneck'], bottlenecks['data_loading_bottleneck']]):
            bottlenecks['recommendations'].append("âœ… æœªå‘ç°æ˜æ˜¾ç“¶é¢ˆï¼Œæ€§èƒ½é…ç½®è¾ƒä¸ºåˆç†")
            
        return bottlenecks
        
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        bottlenecks = self.analyze_bottlenecks()
        
        report_path = self.log_dir / "bottleneck_report.txt"
        with open(report_path, 'w') as f:
            f.write("=== ğŸ” æ™ºèƒ½ç“¶é¢ˆåˆ†ææŠ¥å‘Š ===\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now()}\n\n")
            
            # ç“¶é¢ˆæ€»ç»“
            f.write("ğŸ“Š ç“¶é¢ˆæ£€æµ‹ç»“æœ:\n")
            f.write(f"  GPUç“¶é¢ˆ: {'æ˜¯' if bottlenecks['gpu_bottleneck'] else 'å¦'}\n")
            f.write(f"  CPUç“¶é¢ˆ: {'æ˜¯' if bottlenecks['cpu_bottleneck'] else 'å¦'}\n")
            f.write(f"  å†…å­˜ç“¶é¢ˆ: {'æ˜¯' if bottlenecks['memory_bottleneck'] else 'å¦'}\n")
            f.write(f"  æ•°æ®åŠ è½½ç“¶é¢ˆ: {'æ˜¯' if bottlenecks['data_loading_bottleneck'] else 'å¦'}\n\n")
            
            # æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
            if self.gpu_metrics:
                gpu_utils = [m['utilization'] for m in self.gpu_metrics if m.get('utilization', 0) > 0]
                if gpu_utils:
                    f.write("ğŸ¯ GPUæ€§èƒ½ç»Ÿè®¡:\n")
                    f.write(f"  å¹³å‡åˆ©ç”¨ç‡: {np.mean(gpu_utils):.1f}%\n")
                    f.write(f"  æœ€å¤§åˆ©ç”¨ç‡: {max(gpu_utils):.1f}%\n")
                    f.write(f"  æœ€å°åˆ©ç”¨ç‡: {min(gpu_utils):.1f}%\n")
                    
                gpu_memory = [m['memory_allocated'] for m in self.gpu_metrics]
                f.write(f"  æœ€å¤§GPUå†…å­˜: {max(gpu_memory):.2f}GB\n\n")
                
            if self.cpu_metrics:
                cpu_utils = [m['utilization'] for m in self.cpu_metrics]
                f.write("ğŸ–¥ï¸  CPUæ€§èƒ½ç»Ÿè®¡:\n")
                f.write(f"  å¹³å‡åˆ©ç”¨ç‡: {np.mean(cpu_utils):.1f}%\n")
                f.write(f"  æœ€å¤§åˆ©ç”¨ç‡: {max(cpu_utils):.1f}%\n\n")
                
            # æ—¶é—´åˆ†æ
            f.write("â±ï¸  æ—¶é—´åˆ†æ:\n")
            total_time = sum(self.timings.values())
            for phase, duration in self.timings.items():
                percentage = (duration / total_time * 100) if total_time > 0 else 0
                f.write(f"  {phase}: {duration:.2f}s ({percentage:.1f}%)\n")
            f.write(f"  æ€»æ—¶é—´: {total_time:.2f}s\n\n")
            
            # ä¼˜åŒ–å»ºè®®
            f.write("ğŸš€ ä¼˜åŒ–å»ºè®®:\n")
            for i, rec in enumerate(bottlenecks['recommendations'], 1):
                f.write(f"  {i}. {rec}\n")
                
        print(f"ğŸ“‹ è¯¦ç»†åˆ†ææŠ¥å‘Šä¿å­˜åˆ°: {report_path}")
        
        # ä¿å­˜åŸå§‹æ•°æ®
        self._save_raw_data()
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._generate_plots()
        
        return bottlenecks
        
    def _save_raw_data(self):
        """ä¿å­˜åŸå§‹ç›‘æ§æ•°æ®"""
        data = {
            'gpu_metrics': list(self.gpu_metrics),
            'cpu_metrics': list(self.cpu_metrics),
            'memory_metrics': list(self.memory_metrics),
            'io_metrics': list(self.io_metrics),
            'timings': self.timings
        }
        
        with open(self.profiler_dir / "raw_metrics.json", 'w') as f:
            json.dump(data, f, indent=2, default=str)
            
    def _generate_plots(self):
        """ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # GPUåˆ©ç”¨ç‡å›¾
            if self.gpu_metrics:
                gpu_times = [m['timestamp'] for m in self.gpu_metrics if m.get('utilization', 0) > 0]
                gpu_utils = [m['utilization'] for m in self.gpu_metrics if m.get('utilization', 0) > 0]
                if gpu_times and gpu_utils:
                    axes[0, 0].plot(gpu_times, gpu_utils, 'g-', linewidth=2)
                    axes[0, 0].axhline(y=70, color='r', linestyle='--', label='ç›®æ ‡çº¿ (70%)')
                    axes[0, 0].set_title('GPUåˆ©ç”¨ç‡')
                    axes[0, 0].set_ylabel('åˆ©ç”¨ç‡ (%)')
                    axes[0, 0].legend()
                    axes[0, 0].grid(True)
            
            # CPUåˆ©ç”¨ç‡å›¾
            if self.cpu_metrics:
                cpu_times = [m['timestamp'] for m in self.cpu_metrics]
                cpu_utils = [m['utilization'] for m in self.cpu_metrics]
                axes[0, 1].plot(cpu_times, cpu_utils, 'b-', linewidth=2)
                axes[0, 1].axhline(y=80, color='r', linestyle='--', label='è­¦å‘Šçº¿ (80%)')
                axes[0, 1].set_title('CPUåˆ©ç”¨ç‡')
                axes[0, 1].set_ylabel('åˆ©ç”¨ç‡ (%)')
                axes[0, 1].legend()
                axes[0, 1].grid(True)
            
            # å†…å­˜ä½¿ç”¨å›¾
            if self.memory_metrics:
                mem_times = [m['timestamp'] for m in self.memory_metrics]
                mem_utils = [m['percent'] for m in self.memory_metrics]
                axes[1, 0].plot(mem_times, mem_utils, 'orange', linewidth=2)
                axes[1, 0].axhline(y=85, color='r', linestyle='--', label='è­¦å‘Šçº¿ (85%)')
                axes[1, 0].set_title('å†…å­˜ä½¿ç”¨ç‡')
                axes[1, 0].set_ylabel('ä½¿ç”¨ç‡ (%)')
                axes[1, 0].legend()
                axes[1, 0].grid(True)
            
            # GPUå†…å­˜ä½¿ç”¨å›¾
            if self.gpu_metrics:
                gpu_mem_times = [m['timestamp'] for m in self.gpu_metrics]
                gpu_mem_usage = [m['memory_allocated'] for m in self.gpu_metrics]
                axes[1, 1].plot(gpu_mem_times, gpu_mem_usage, 'purple', linewidth=2)
                axes[1, 1].set_title('GPUå†…å­˜ä½¿ç”¨')
                axes[1, 1].set_ylabel('å†…å­˜ (GB)')
                axes[1, 1].grid(True)
            
            plt.tight_layout()
            plt.savefig(self.profiler_dir / "performance_charts.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“ˆ æ€§èƒ½å›¾è¡¨ä¿å­˜åˆ°: {self.profiler_dir}/performance_charts.png")
            
        except Exception as e:
            print(f"ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")

# -----------------------------------------------------------------------------
# ç®€åŒ–çš„è®­ç»ƒè„šæœ¬
# -----------------------------------------------------------------------------

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

def create_log_directory(base_dir: str = "logs") -> Path:
    """åˆ›å»ºæ—¥å¿—ç›®å½•"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = Path(base_dir) / f"bottleneck_analysis_{timestamp}"
    log_dir.mkdir(parents=True, exist_ok=True)
    return log_dir

def load_dataset(x_path: str, y_path: str) -> Tuple[torch.Tensor, torch.Tensor]:
    """åŠ è½½æ•°æ®é›†"""
    x_df = pd.read_csv(x_path)
    y_df = pd.read_csv(y_path)
    
    n = min(len(x_df), len(y_df))
    x_df = x_df.iloc[:n]
    y_df = y_df.iloc[:n]
    
    x_tensor = torch.tensor(x_df.values, dtype=torch.float32)
    y_tensor = torch.tensor(y_df.values.reshape(-1, 1), dtype=torch.float32)
    
    return x_tensor, y_tensor

def build_nfs_model(context_features: int, flow_features: int = 1, 
                   hidden_features: int = 128, n_layers: int = 10) -> Flow:
    """æ„å»ºå½’ä¸€åŒ–æµæ¨¡å‹"""
    base_dist = StandardNormal([flow_features])
    transforms: List = []
    for _ in range(n_layers):
        transforms += [
            RandomPermutation(features=flow_features),
            MaskedAffineAutoregressiveTransform(
                features=flow_features,
                hidden_features=hidden_features,
                context_features=context_features,
            ),
        ]
    return Flow(CompositeTransform(transforms), base_dist)

def train_with_bottleneck_analysis(model: Flow, train_loader: DataLoader, 
                                 analyzer: BottleneckAnalyzer, epochs: int = 50, 
                                 lr: float = 1e-3) -> List[float]:
    """å¸¦ç“¶é¢ˆåˆ†æçš„è®­ç»ƒå‡½æ•°"""
    model.to(device)
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    # æ··åˆç²¾åº¦
    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None
    
    losses = []
    
    print(f"ğŸ¯ è®­ç»ƒé…ç½®:")
    print(f"   Epochs: {epochs}")
    print(f"   Batch size: {train_loader.batch_size}")
    print(f"   Batches per epoch: {len(train_loader)}")
    print(f"   æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   æ··åˆç²¾åº¦: {'å¯ç”¨' if scaler else 'ç¦ç”¨'}")
    
    with analyzer.time_phase("è®­ç»ƒ"):
        for epoch in range(1, epochs + 1):
            model.train()
            epoch_loss = 0.0
            batch_count = 0
            
            # æ¯10ä¸ªepochåˆ†æä¸€æ¬¡ç“¶é¢ˆ
            if epoch % 10 == 1:
                print(f"\nğŸ“Š Epoch {epoch} - å®æ—¶ç“¶é¢ˆåˆ†æ:")
                current_bottlenecks = analyzer.analyze_bottlenecks()
                for rec in current_bottlenecks['recommendations'][:3]:  # æ˜¾ç¤ºå‰3ä¸ªå»ºè®®
                    print(f"   {rec}")
            
            for batch_idx, (cx, y) in enumerate(train_loader):
                # æ•°æ®ä¼ è¾“åˆ°GPU
                cx = cx.to(device, non_blocking=True)
                y = y.to(device, non_blocking=True)
                
                optimizer.zero_grad(set_to_none=True)
                
                # å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
                if scaler:
                    with torch.amp.autocast('cuda'):
                        loss = -model.log_prob(inputs=y, context=cx).mean()
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    loss = -model.log_prob(inputs=y, context=cx).mean()
                    loss.backward()
                    optimizer.step()
                
                epoch_loss += loss.item()
                batch_count += 1
            
            epoch_loss /= batch_count
            losses.append(epoch_loss)
            scheduler.step()
            
            # å®šæœŸè¾“å‡ºè¿›åº¦
            if epoch % 10 == 0 or epoch == 1:
                current_lr = scheduler.get_last_lr()[0]
                print(f"Epoch {epoch:3d}/{epochs} | Loss: {epoch_loss:.4f} | LR: {current_lr:.6f}")
                
                # æ˜¾ç¤ºå½“å‰èµ„æºä½¿ç”¨
                if analyzer.gpu_metrics:
                    latest_gpu = analyzer.gpu_metrics[-1]
                    print(f"   GPU: {latest_gpu.get('utilization', 0):.1f}% | "
                          f"Memory: {latest_gpu['memory_allocated']:.1f}GB")
                    
                if analyzer.cpu_metrics:
                    latest_cpu = analyzer.cpu_metrics[-1]
                    print(f"   CPU: {latest_cpu['utilization']:.1f}% | "
                          f"RAM: {analyzer.memory_metrics[-1]['percent']:.1f}%")
    
    return losses

def main():
    parser = argparse.ArgumentParser(description="æ™ºèƒ½ç“¶é¢ˆåˆ†æè®­ç»ƒ")
    parser.add_argument("--x_csv", type=str, default="data/x_data.csv", help="è¾“å…¥ç‰¹å¾æ–‡ä»¶")
    parser.add_argument("--y_csv", type=str, default="data/y_data.csv", help="ç›®æ ‡å€¼æ–‡ä»¶")
    parser.add_argument("--epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=2048, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--hidden_features", type=int, default=128, help="éšè—å±‚å¤§å°")
    parser.add_argument("--n_layers", type=int, default=10, help="æ¨¡å‹å±‚æ•°")
    parser.add_argument("--train_ratio", type=float, default=0.8, help="è®­ç»ƒé›†æ¯”ä¾‹")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½è¿›ç¨‹æ•°")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•å’Œåˆ†æå™¨
    log_dir = create_log_directory()
    analyzer = BottleneckAnalyzer(log_dir, sample_interval=0.5)
    
    print(f"ğŸ“ åˆ†æç»“æœå°†ä¿å­˜åˆ°: {log_dir}")
    print(f"ğŸ–¥ï¸  ç³»ç»Ÿä¿¡æ¯: {psutil.cpu_count()} CPUs, {psutil.virtual_memory().total/1024**3:.1f}GB RAM")
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ¯ GPU: {gpu_name} ({gpu_memory:.1f}GB)")
    
    try:
        # å¼€å§‹ç›‘æ§
        analyzer.start_monitoring()
        
        # åŠ è½½æ•°æ®
        with analyzer.time_phase("æ•°æ®åŠ è½½"):
            x, y = load_dataset(args.x_csv, args.y_csv)
            print(f"ğŸ“Š æ•°æ®é›†: {len(x)} æ ·æœ¬, {x.shape[1]} ç‰¹å¾")
            
            # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            n_train = int(len(x) * args.train_ratio)
            indices = torch.randperm(len(x))
            train_indices = indices[:n_train]
            test_indices = indices[n_train:]
            
            x_train, y_train = x[train_indices], y[train_indices]
            x_test, y_test = x[test_indices], y[test_indices]
            
            print(f"ğŸ“¦ è®­ç»ƒé›†: {len(x_train)} æ ·æœ¬, æµ‹è¯•é›†: {len(x_test)} æ ·æœ¬")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        with analyzer.time_phase("æ•°æ®åŠ è½½å™¨åˆ›å»º"):
            train_dataset = TensorDataset(x_train, y_train)
            train_loader = DataLoader(
                train_dataset,
                batch_size=args.batch_size,
                shuffle=True,
                num_workers=args.num_workers,
                pin_memory=torch.cuda.is_available(),
                persistent_workers=True if args.num_workers > 0 else False
            )
            
            print(f"ğŸ“¦ DataLoaderé…ç½®:")
            print(f"   Batch size: {args.batch_size}")
            print(f"   Num workers: {args.num_workers}")
            print(f"   Pin memory: {torch.cuda.is_available()}")
        
        # æ„å»ºæ¨¡å‹
        with analyzer.time_phase("æ¨¡å‹æ„å»º"):
            model = build_nfs_model(
                context_features=x.shape[1],
                hidden_features=args.hidden_features,
                n_layers=args.n_layers
            )
            
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"ğŸ§  æ¨¡å‹ä¿¡æ¯:")
            print(f"   æ€»å‚æ•°: {total_params:,}")
            print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            print(f"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f}MB")
        
        # è®­ç»ƒæ¨¡å‹
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        losses = train_with_bottleneck_analysis(
            model, train_loader, analyzer,
            epochs=args.epochs
        )
        
        print(f"âœ… è®­ç»ƒå®Œæˆ! æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
        
        # åœæ­¢ç›‘æ§
        analyzer.stop_monitoring()
        
        # ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
        print(f"\nğŸ” ç”Ÿæˆç“¶é¢ˆåˆ†ææŠ¥å‘Š...")
        bottlenecks = analyzer.generate_comprehensive_report()
        
        # è¾“å‡ºå…³é”®å‘ç°
        print(f"\n=== ğŸ¯ ç“¶é¢ˆåˆ†æç»“æœ ===")
        print(f"GPUç“¶é¢ˆ: {'âš ï¸  æ˜¯' if bottlenecks['gpu_bottleneck'] else 'âœ… å¦'}")
        print(f"CPUç“¶é¢ˆ: {'âš ï¸  æ˜¯' if bottlenecks['cpu_bottleneck'] else 'âœ… å¦'}")
        print(f"å†…å­˜ç“¶é¢ˆ: {'âš ï¸  æ˜¯' if bottlenecks['memory_bottleneck'] else 'âœ… å¦'}")
        print(f"æ•°æ®åŠ è½½ç“¶é¢ˆ: {'âš ï¸  æ˜¯' if bottlenecks['data_loading_bottleneck'] else 'âœ… å¦'}")
        
        print(f"\nğŸš€ ä¼˜åŒ–å»ºè®®:")
        for i, rec in enumerate(bottlenecks['recommendations'], 1):
            print(f"  {i}. {rec}")
        
        # ä¿å­˜æ¨¡å‹
        model_path = log_dir / "trained_model.pt"
        torch.save({
            'model_state_dict': model.state_dict(),
            'losses': losses,
            'config': {
                'context_features': x.shape[1],
                'hidden_features': args.hidden_features,
                'n_layers': args.n_layers
            },
            'bottleneck_analysis': bottlenecks
        }, model_path)
        
        print(f"\nğŸ’¾ æ¨¡å‹ä¿å­˜åˆ°: {model_path}")
        print(f"ğŸ“‹ å®Œæ•´æŠ¥å‘Š: {log_dir}/bottleneck_report.txt")
        print(f"ğŸ“ˆ æ€§èƒ½å›¾è¡¨: {log_dir}/bottleneck_analysis/performance_charts.png")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        analyzer.stop_monitoring()

if __name__ == "__main__":
    main()